import numpy as np
import random


class ChessBoard:
    def __init__(self, size):
        self.size = size
        self.board_array = np.array([[None] * size for _ in range(size)])
        self.turn = 0

    def check(self):
        # Check rows, columns, and diagonals for a win
        for i in range(self.size):
            if self.board_array[i][0] and len(set(self.board_array[i])) == 1:
                return self.board_array[i][0]
            if (
                self.board_array[0][i]
                and len(set([row[i] for row in self.board_array])) == 1
            ):
                return self.board_array[0][i]
        if (
            self.board_array[0][0]
            and len(set([self.board_array[i][i] for i in range(self.size)])) == 1
        ):
            return self.board_array[0][0]
        if (
            self.board_array[0][self.size - 1]
            and len(
                set([self.board_array[i][self.size - 1 - i] for i in range(self.size)])
            )
            == 1
        ):
            return self.board_array[0][self.size - 1]
        return None

    def is_full(self):
        return all(all(cell is not None for cell in row) for row in self.board_array)

    def print_board(self):
        for row in self.board_array:
            print("|".join([cell if cell else " " for cell in row]))


class Agent:
    def __init__(self, char, epsilon=0.1, alpha=0.1, gamma=0.9):
        self.char = char
        self.epsilon = epsilon
        self.alpha = alpha
        self.gamma = gamma
        self.q_table = {}

    def get_state_key(self, state):
        # Convert the state to a flattened tuple for better representation
        return tuple(map(tuple, state))

    def choose_action(self, state, valid_actions):
        if random.random() < self.epsilon:
            return random.choice(valid_actions)
        else:
            state_key = self.get_state_key(state)
            q_values = [
                self.q_table.get((state_key, tuple(action)), 0)
                for action in valid_actions
            ]
            max_q = max(q_values)
            if q_values.count(max_q) > 1:
                best_actions = [
                    action
                    for i, action in enumerate(valid_actions)
                    if q_values[i] == max_q
                ]
                return random.choice(best_actions)
            else:
                return valid_actions[q_values.index(max_q)]

    def update_q_value(self, state, action, reward, next_state):
        state_key = self.get_state_key(state)
        next_state_key = self.get_state_key(next_state)
        valid_actions = self.get_valid_actions(next_state)
        if valid_actions:
            next_max = max(
                [self.q_table.get((next_state_key, tuple(a)), 0) for a in valid_actions]
            )
        else:
            next_max = 0
        self.q_table[(state_key, tuple(action))] = self.q_table.get(
            (state_key, tuple(action)), 0
        ) + self.alpha * (
            reward
            + self.gamma * next_max
            - self.q_table.get((state_key, tuple(action)), 0)
        )

    def get_valid_actions(self, state):
        return [
            (i, j)
            for i in range(len(state))
            for j in range(len(state[i]))
            if state[i][j] is None
        ]


class Game:
    def __init__(self):
        self.chess_board = ChessBoard(3)
        self.player1 = Agent("X", epsilon=1.0)  # X is a random player (epsilon=1.0)
        self.player2 = Agent("O", epsilon=0.1)  # O is the RL agent
        self.results = {"X": 0, "O": 0, "tie": 0}  # Track wins and ties

    def run(self, episodes=1000):
        for episode in range(episodes):
            self.chess_board = ChessBoard(3)
            while True:
                # X's turn (random player)
                state = self.chess_board.board_array
                valid_actions = self.player1.get_valid_actions(state)
                action = self.player1.choose_action(state, valid_actions)
                self.chess_board.board_array[action[0]][action[1]] = self.player1.char
                winner = self.chess_board.check()
                if winner:
                    reward = -1 if winner == self.player1.char else 1
                    self.player2.update_q_value(
                        state, action, reward, self.chess_board.board_array
                    )
                    self.results[winner] += 1  # Update results
                    break
                if self.chess_board.is_full():
                    self.player2.update_q_value(
                        state, action, 0, self.chess_board.board_array
                    )
                    self.results["tie"] += 1  # Update results
                    break

                # O's turn (RL agent)
                state = self.chess_board.board_array
                valid_actions = self.player2.get_valid_actions(state)
                action = self.player2.choose_action(state, valid_actions)
                self.chess_board.board_array[action[0]][action[1]] = self.player2.char
                winner = self.chess_board.check()
                if winner:
                    reward = 1 if winner == self.player2.char else -1
                    self.player2.update_q_value(
                        state, action, reward, self.chess_board.board_array
                    )
                    self.results[winner] += 1  # Update results
                    break
                if self.chess_board.is_full():
                    self.player2.update_q_value(
                        state, action, 0, self.chess_board.board_array
                    )
                    self.results["tie"] += 1  # Update results
                    break

    def test(self, episodes=100):
        self.results = {"X": 0, "O": 0, "tie": 0}

        for episode in range(episodes):
            self.chess_board = ChessBoard(3)
            while True:
                # X's turn (random player)
                state = self.chess_board.board_array
                valid_actions = self.player1.get_valid_actions(state)
                action = self.player1.choose_action(state, valid_actions)
                self.chess_board.board_array[action[0]][action[1]] = self.player1.char
                winner = self.chess_board.check()
                if winner:
                    # reward = -1 if winner == self.player1.char else 1
                    # self.player2.update_q_value(
                    #     state, action, reward, self.chess_board.board_array
                    # )
                    self.results[winner] += 1  # Update results
                    break
                if self.chess_board.is_full():
                    # self.player2.update_q_value(
                    #     state, action, 0, self.chess_board.board_array
                    # )
                    self.results["tie"] += 1  # Update results
                    break

                # O's turn (RL agent)
                state = self.chess_board.board_array
                valid_actions = self.player2.get_valid_actions(state)
                action = self.player2.choose_action(state, valid_actions)
                self.chess_board.board_array[action[0]][action[1]] = self.player2.char
                winner = self.chess_board.check()
                if winner:
                    # reward = 1 if winner == self.player2.char else -1
                    # self.player2.update_q_value(
                    #     state, action, reward, self.chess_board.board_array
                    # )
                    self.results[winner] += 1  # Update results
                    break
                if self.chess_board.is_full():
                    # self.player2.update_q_value(
                    #     state, action, 0, self.chess_board.board_array
                    # )
                    self.results["tie"] += 1  # Update results
                    break

    def print_results(self):
        total_games = sum(self.results.values())
        print(f"Total Games Played: {total_games}")
        print(
            f"X Wins: {self.results['X']} ({self.results['X'] / total_games * 100:.2f}%)"
        )
        print(
            f"O Wins: {self.results['O']} ({self.results['O'] / total_games * 100:.2f}%)"
        )
        print(
            f"Ties: {self.results['tie']} ({self.results['tie'] / total_games * 100:.2f}%)"
        )


if __name__ == "__main__":
    game = Game()
    game.run(episodes=100000)
    print("train results")
    game.print_results()

    game.test()
    print("test results")
    game.print_results()


# train results
# Total Games Played: 100000
# X Wins: 58628 (58.63%)
# O Wins: 28674 (28.67%)
# Ties: 12698 (12.70%)
# test results
# Total Games Played: 100
# X Wins: 66 (66.00%)
# O Wins: 20 (20.00%)
# Ties: 14 (14.00%)
